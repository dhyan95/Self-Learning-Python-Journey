# -*- coding: utf-8 -*-
"""Credit Risk Scoring & Stability Analysis (Real-World, NumPy-centric).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d7E9kTUeGImFBMRD-aUT7XoNdYYvKHeF
"""

import numpy as np

"""CREATING A DATASET"""

import numpy as np

np.random.seed(42)
n = 10_000

age = np.random.normal(loc=40, scale=10, size=n)
age = np.clip(age, 21, 70)

income = np.random.lognormal(mean=10.5, sigma=0.5, size=n)

debt = income * np.random.uniform(0.1, 0.6, size=n)

credit_utilization = (
    0.3 * (debt / income) +
    0.7 * np.random.beta(a=2, b=5, size=n)
)
credit_utilization = np.clip(credit_utilization, 0, 1)

late_payments = np.random.poisson(lam=2, size=n)

monthly_spend = (
    0.4 * income / 12 +
    0.6 * debt / 12 +
    np.random.normal(0, 500, size=n)
)
monthly_spend = np.clip(monthly_spend, 0, None)

X = np.column_stack([
    income,
    debt,
    credit_utilization,
    late_payments,
    monthly_spend,
    age
])

mask = np.random.rand(*X.shape) < 0.05
X[mask] = np.nan

X

"""Pre-Processing - NumPy basics (arrays, masking, broadcasting)"""

X.shape

np.isnan(X).sum(axis=0)

mean = np.nanmean(X,axis=0)   #mean ignoring NaN
mean

std = np.nanstd(X,axis=0)      #std ignoring NaN
std

"""Always clip before normalizing"""

#clip outliers using percentiles
#Method - 1 : IQR
Q1 = np.nanpercentile(X,25,axis=0)
Q3 = np.nanpercentile(X,75,axis=0)
IQR = Q3-Q1
LB = Q1 - 1.5*IQR
UB = Q3 + 1.5*IQR
X_clip = np.clip(X,LB,UB)
print("Lower bound: ",LB)
print("Upper bound: ",UB)
print("Clipped data: ",X_clip)

#broadcasting to normalize the data (Z-Score method)
norm = (X_clip-mean)/std
norm_mean = np.nanmean(norm,axis=0)
print("Mean: ",mean)
print("Normalized mean: ",norm_mean)

"""Statistics (distribution & relationships)"""

#mean
mean_norm = np.nanmean(norm,axis=0)
print("Mean: ",mean_norm)
#std
std_norm = np.nanstd(norm,axis=0)
print("Standard deviation: ",std_norm)

col_mean = np.nanmean(norm, axis=0)
X_filled = np.where(np.isnan(norm), col_mean, norm)
cov = np.cov(X_filled, rowvar=False)   #each column is a feature and row is the observation - male vs height,weight,age.
corr = np.corrcoef(X_filled, rowvar=False)
print("Covariance matrix: \n",cov)     #risk of investment - both are moving together? moving together - no issue, else issue
print("\n")
print("Correlation Coefficient: \n",corr)     #portfolio diversification - 1(perfectly linked), 0(no linkage), -1(perfectly opposite)

"""Linear algebra (the core engine)"""

#risk score as a weighted linear combination
#giving each feature a risk.
#income,debt,credit_utilization,late_payments,monthly_spend,age
risk_weights = np.array([0.5,0.8,0.1,0.7,0.3,0.2])
risk_score = X_filled @ risk_weights
risk_score
#positive weight - increases risk
#negative weight - decreases risk

#Solve weights using least squares (lstsq) against a synthetic “default risk” target
#Least Square method works better after pre-processing.
#learning weights from historical data.

#Creating synthetic weights
true_weights = np.array([0.6,0.9,0.2,0.8,0.4,-0.3])
noise = 0.2 * np.random.randn(X_filled.shape[0])   #how risk behaves in history
y = X_filled @ true_weights + noise
y

#Solve for weights using Least Squares
#Using lstsq, rank, singular values, or conditioning is SVD usage
w_lstsq, residuals, rank, s = np.linalg.lstsq(X_filled,y,rcond=None)  #rcond=None “Ignore extremely tiny, noisy directions.”
print("Learned weights:", w_lstsq)
print("Residual sum of squares:", residuals)

#IMPORTANT - Old vs New
print("Manual weights :", risk_weights)
print("Lstsq weights  :", w_lstsq)

#Check numerical stability - if this number is large donot trust LSTSQ
num_sta = np.linalg.cond(X_filled)
num_sta

#Singular values - undestand instability;
#Large values → strong independent signals
#Very small values → redundancy
print("Singular values: ",s)

#Stabilize weights using truncated SVD
#You removed noisy, redundant directions.
k = 4  # keep strongest directions instead of the 6 features
X_reduced = X_filled @ Vt[:k, :].T
#Vt = a rotation map
#It turns messy features into clean ideas
#[:k] = keep only the strongest ideas
#@ = project data onto those ideas

risk_score_lstsq = X_filled @ w_lstsq

np.corrcoef(risk_score, risk_score_lstsq)[0,1]
#High correlation → manual score was good.
#Low correlation → model learned new structure.

#Your manual weights were almost correct
#The data agrees with your intuition
#The model did not discover something totally new